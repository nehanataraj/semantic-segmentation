{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+XWSeQBXUN6t7OSbLO02C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehanataraj/semantic-segmentation/blob/main/Object_detection_w_pytorch_(final_project).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X2Q1Gc4Swop"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import time\n",
        "import shutil\n",
        "import torch\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils import data\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from ptsemseg.models import get_model\n",
        "# from ptsemseg.loss import get_loss_function\n",
        "# from ptsemseg.loader import get_loader\n",
        "# from ptsemseg.utils import get_logger\n",
        "# from ptsemseg.metrics import runningScore, averageMeter\n",
        "# from ptsemseg.augmentations import get_composed_augmentations\n",
        "# from ptsemseg.schedulers import get_scheduler\n",
        "# from ptsemseg.optimizers import get_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3l6BuRBWPgl",
        "outputId": "5523cbff-0ccc-4dc4-b940-4a9f5fd9530b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (23.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboardX import SummaryWriter"
      ],
      "metadata": {
        "id": "XO1jgzLeW__l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(cfg, writer, logger):\n",
        "\n",
        "    # Setup seeds\n",
        "    torch.manual_seed( 1337)\n",
        "    torch.cuda.manual_seed(1337)\n",
        "    np.random.seed(1337)\n",
        "    random.seed(1337)\n",
        "\n",
        "    # Setup device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Setup Augmentations\n",
        "    augmentations = cfg[\"training\"].get(\"augmentations\", None)\n",
        "    data_aug = get_composed_augmentations(augmentations)\n",
        "\n",
        "    # Setup Dataloader\n",
        "    data_loader = get_loader(cfg[\"data\"][\"dataset\"])\n",
        "    data_path = cfg[\"data\"][\"path\"]\n",
        "\n",
        "    t_loader = data_loader(\n",
        "        data_path,\n",
        "        is_transform=True,\n",
        "        split=cfg[\"data\"][\"train_split\"],\n",
        "        img_size=(cfg[\"data\"][\"img_rows\"], cfg[\"data\"][\"img_cols\"]),\n",
        "        augmentations=data_aug,\n",
        "    )\n",
        "\n",
        "    v_loader = data_loader(\n",
        "        data_path,\n",
        "        is_transform=True,\n",
        "        split=cfg[\"data\"][\"val_split\"],\n",
        "        img_size=(cfg[\"data\"][\"img_rows\"], cfg[\"data\"][\"img_cols\"]),\n",
        "    )\n",
        "\n",
        "    n_classes = t_loader.n_classes\n",
        "    trainloader = data.DataLoader(\n",
        "        t_loader,\n",
        "        batch_size=cfg[\"training\"][\"batch_size\"],\n",
        "        num_workers=cfg[\"training\"][\"n_workers\"],\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    valloader = data.DataLoader(\n",
        "        v_loader, batch_size=cfg[\"training\"][\"batch_size\"], num_workers=cfg[\"training\"][\"n_workers\"]\n",
        "    )\n",
        "\n",
        "    # Setup Metrics\n",
        "    running_metrics_val = runningScore(n_classes)\n",
        "\n",
        "    # Setup Model\n",
        "    model = get_model(cfg[\"model\"], n_classes).to(device)\n",
        "\n",
        "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
        "\n",
        "    # Setup optimizer, lr_scheduler and loss function\n",
        "    optimizer_cls = get_optimizer(cfg)\n",
        "    optimizer_params = {k: v for k, v in cfg[\"training\"][\"optimizer\"].items() if k != \"name\"}\n",
        "\n",
        "    optimizer = optimizer_cls(model.parameters(), **optimizer_params)\n",
        "    logger.info(\"Using optimizer {}\".format(optimizer))\n",
        "\n",
        "    scheduler = get_scheduler(optimizer, cfg[\"training\"][\"lr_schedule\"])\n",
        "\n",
        "    loss_fn = get_loss_function(cfg)\n",
        "    logger.info(\"Using loss {}\".format(loss_fn))\n",
        "\n",
        "    start_iter = 0\n",
        "    if cfg[\"training\"][\"resume\"] is not None:\n",
        "        if os.path.isfile(cfg[\"training\"][\"resume\"]):\n",
        "            logger.info(\n",
        "                \"Loading model and optimizer from checkpoint '{}'\".format(cfg[\"training\"][\"resume\"])\n",
        "            )\n",
        "            checkpoint = torch.load(cfg[\"training\"][\"resume\"])\n",
        "            model.load_state_dict(checkpoint[\"model_state\"])\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
        "            start_iter = checkpoint[\"epoch\"]\n",
        "            logger.info(\n",
        "                \"Loaded checkpoint '{}' (iter {})\".format(\n",
        "                    cfg[\"training\"][\"resume\"], checkpoint[\"epoch\"]\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            logger.info(\"No checkpoint found at '{}'\".format(cfg[\"training\"][\"resume\"]))\n",
        "\n",
        "    val_loss_meter = averageMeter()\n",
        "    time_meter = averageMeter()\n",
        "\n",
        "    best_iou = -100.0\n",
        "    i = start_iter\n",
        "    flag = True\n",
        "\n",
        "    while i <= cfg[\"training\"][\"train_iters\"] and flag:\n",
        "        for (images, labels) in trainloader:\n",
        "            i += 1\n",
        "            start_ts = time.time()\n",
        "            scheduler.step()\n",
        "            model.train()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = loss_fn(input=outputs, target=labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            time_meter.update(time.time() - start_ts)\n",
        "\n",
        "            if (i + 1) % cfg[\"training\"][\"print_interval\"] == 0:\n",
        "                fmt_str = \"Iter [{:d}/{:d}]  Loss: {:.4f}  Time/Image: {:.4f}\"\n",
        "                print_str = fmt_str.format(\n",
        "                    i + 1,\n",
        "                    cfg[\"training\"][\"train_iters\"],\n",
        "                    loss.item(),\n",
        "                    time_meter.avg / cfg[\"training\"][\"batch_size\"],\n",
        "                )\n",
        "\n",
        "                print(print_str)\n",
        "                logger.info(print_str)\n",
        "                writer.add_scalar(\"loss/train_loss\", loss.item(), i + 1)\n",
        "                time_meter.reset()\n",
        "\n",
        "            if (i + 1) % cfg[\"training\"][\"val_interval\"] == 0 or (i + 1) == cfg[\"training\"][\n",
        "                \"train_iters\"\n",
        "            ]:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for i_val, (images_val, labels_val) in tqdm(enumerate(valloader)):\n",
        "                        images_val = images_val.to(device)\n",
        "                        labels_val = labels_val.to(device)\n",
        "\n",
        "                        outputs = model(images_val)\n",
        "                        val_loss = loss_fn(input=outputs, target=labels_val)\n",
        "\n",
        "                        pred = outputs.data.max(1)[1].cpu().numpy()\n",
        "                        gt = labels_val.data.cpu().numpy()\n",
        "\n",
        "                        running_metrics_val.update(gt, pred)\n",
        "                        val_loss_meter.update(val_loss.item())\n",
        "\n",
        "                writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n",
        "                logger.info(\"Iter %d Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n",
        "\n",
        "                score, class_iou = running_metrics_val.get_scores()\n",
        "                for k, v in score.items():\n",
        "                    print(k, v)\n",
        "                    logger.info(\"{}: {}\".format(k, v))\n",
        "                    writer.add_scalar(\"val_metrics/{}\".format(k), v, i + 1)\n",
        "\n",
        "                for k, v in class_iou.items():\n",
        "                    logger.info(\"{}: {}\".format(k, v))\n",
        "                    writer.add_scalar(\"val_metrics/cls_{}\".format(k), v, i + 1)\n",
        "\n",
        "                val_loss_meter.reset()\n",
        "                running_metrics_val.reset()\n",
        "\n",
        "                if score[\"Mean IoU : \\t\"] >= best_iou:\n",
        "                    best_iou = score[\"Mean IoU : \\t\"]\n",
        "                    state = {\n",
        "                        \"epoch\": i + 1,\n",
        "                        \"model_state\": model.state_dict(),\n",
        "                        \"optimizer_state\": optimizer.state_dict(),\n",
        "                        \"scheduler_state\": scheduler.state_dict(),\n",
        "                        \"best_iou\": best_iou,\n",
        "                    }\n",
        "                    save_path = os.path.join(\n",
        "                        writer.file_writer.get_logdir(),\n",
        "                        \"{}_{}_best_model.pkl\".format(cfg[\"model\"][\"arch\"], cfg[\"data\"][\"dataset\"]),\n",
        "                    )\n",
        "                    torch.save(state, save_path)\n",
        "\n",
        "            if (i + 1) == cfg[\"training\"][\"train_iters\"]:\n",
        "                flag = False\n",
        "                break\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"config\")\n",
        "    parser.add_argument(\n",
        "        \"--config\",\n",
        "        nargs=\"?\",\n",
        "        type=str,\n",
        "        default=\"configs/fcn8s_pascal.yml\",\n",
        "        help=\"Configuration file to use\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    with open(args.config) as fp:\n",
        "        cfg = yaml.load(fp)\n",
        "\n",
        "    run_id = random.randint(1, 100000)\n",
        "    logdir = os.path.join(\"runs\", os.path.basename(args.config)[:-4], str(run_id))\n",
        "    writer = SummaryWriter(log_dir=logdir)\n",
        "\n",
        "    print(\"RUNDIR: {}\".format(logdir))\n",
        "    shutil.copy(args.config, logdir)\n",
        "\n",
        "    logger = get_logger(logdir)\n",
        "    logger.info(\"Let the games begin\")\n",
        "\n",
        "    train(cfg, writer, logger)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "ciPr0clTXAXJ",
        "outputId": "7aa1cc6b-3d19-43bd-97ec-1e911dae60e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--config [CONFIG]]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-aa9746f4-2bef-46a4-94a0-e143a48cf012.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ljTcO5zcXabb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}